# [数据库高可用方案][0]

## 低读低写并发、低数据量方案

### 方案一：双机高可用方案

##### 1.数据库架构图

![][1]

##### 2.特点

 一台机器 A 作为读写库，另一台 B 作为备份库； A 库故障后 B 库作为读写库； A 库恢复后 A 作为备库。

##### 3.开发说明

此种情况下，数据源配置中的数据库 IP 地址，可采用虚拟的 IP 地址。虚拟 IP 地址由两台数据库机器上的 keepalive 配置，并互相检测心跳。当其中一台故障后，虚拟 IP 地址会自动漂移到另外一台正常的库上。

数据库的主备配置、故障排除和数据补全，需要 DBA 和运维人员来维护。而程序代码或配置并不需要修改。

具体配置可参考资料：

http://lizhenliang.blog.51cto.com/7876557/1362313

http://database.51cto.com/art/201012/237204.htm

http://gaoke.iteye.com/blog/2283890

##### 4.适应场景

读和写都不高的场景（单表数据低于 500 万），双机高可用。

##### 5.优缺点

优点是一个机器故障了可以自动切换；缺点是只有一个库在工作，读写并未分离，并发有限制。

### 方案二：主从结构方案

##### 1.数据库架构图

![][2]

##### 2.特点

 一台机器 A 作为写库，另一台 B 作为读库； A 库故障后 B 库充当读写， A 修复后， B 库为写库， A 库为读库。

##### 3.开发说明

这种方案的实现，要借助数据库中间件 Mycat 来实现， Mycat 的 datahost 配置如下（注意 balance 和 writetype 的设置）

    <dataHost name="localhost1" maxCon="1000" minCon="10" balance="1" writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100">

    <heartbeat>select user()</heartbeat>

    <!-- 主，用于写 -->

    <writeHost host="hostM1" url="192.168.1.135:3306" user="root" password="123" ></writeHost>

    <!-- 主 2 ，用于读 ,hostM1 down 了，自动切换为主，读写都可以 -->

    <writeHost host="hostM2" url="192.168.1.136:3306" user="root" password="123" ></writeHost>

    </dataHost>

项目开发中，要配置 Mycat 数据源，并实现对 Mycat 数据源的数据操作。数据库 A 和数据库 B 应该互为主从。数据库的主主配置、故障排除和数据补全，依然需要 DBA 和运维人员来维护。

##### 4.适应场景

读和写都不是非常高的场景（单表数据低于 1000 万），高可用。比方案一并发要高很多。

##### 5.优缺点

优点是一个机器故障了可以自动切换；读写分离，并发有了很大的提升。缺点是引入了一个 Mycat 节点，若要高可用需要引入至少两个 Mycat 。常规的解决方案是引入 haproxy 和 keepalive 对 mycat 做集群。

![][3]

## 高读低写并发、低数据量方案

### 方案三：一主多从 + 读写分离

##### 1.数据库架构图

![][4]

##### 2.特点

一个主写库 A 多个从库，当主库 A 故障时，提升从库 B 为主写库，同时修改 C 、 D 库为 B 的从库。 A 故障修复后，作为 B 的从库。

##### 3.开发说明

项目开发中需要使用 Mycat 作为中间件，来配置主库和从库，核心配置如下：

    <dataHost name="localhost1" maxCon="1000" minCon="10" balance="1" writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100">

    <heartbeat>select user()</heartbeat>

    <!-- 主 A ，用于写 -->

    <writeHost host="hostM1" url="192.168.1.135:3306" user="root" password="123" ></writeHost>

    <!—从B ，用于读 ,hostM1 down 了，自动切换为主 -->

    <writeHost host="hostM2" url="192.168.1.136:3306" user="root" password="123456" ></writeHost>

    <!—从C ，用于读 -->

    <writeHost host="hostM3" url="192.168.1.137:3306" user="root" password="123" ></writeHost>

    <!—从D ，用于读 -->

    <writeHost host="hostM4" url="192.168.1.138:3306" user="root" password="123" ></writeHost>

    </dataHost>

主库 A 故障后， Mycat 会自动把从 B 提升为写库。而 C 、 D 从库，则可以通过 MHA 等工具，自动修改其主库为 B 。进而实现自动切换的目地。

MHA Manager 可以单独部署在一台独立的机器上管理多个 master-slave 集群，也可以部署在一台 slave 节点上。 MHA Node 运行在每台 MySQL 服务器上， MHA Manager 会定时探测集群中的 master 节点，当 master 出现故障时，它可以自动将最新数据的 slave 提升为新的 master ，然后将所有其他的 slave 重新指向新的 master 。整个故障转移过程对应用程序完全透明。

MHA 相关知识请参考：

http://www.cnblogs.com/gomysql/p/3675429.html

##### 4.适应场景

 该架构适合写并发不大、但是读并发大的很的场景

##### 5.优缺点

由于配置了多个读节点，读并发的能力有了质的提高。理论上来说，读节点可以多个，可以负载很高级别的读并发。当然， Mycat 依然需要设计高可用方案。

## 高读写并发、低数据量方案

### 方案四：MariaDB Galera Cluster 方案

##### 1.数据库架构图

![][5]

##### 2.特点

多个数据库，在负载均衡作用下，可同时进行写入和读取操作；各个库之间以 Galera Replication 的方法进行数据同步，即每个库理论上来说，数据是完全一致的。

##### 3.开发说明

 数据库读写时，只需要修改数据库读写 IP 为 keepalive 的虚拟节点即可；数据库配置方面相对比较复杂，需要引入 haproxy 、 keepalive 、 Galaera 等各种插件和配置。

##### 4.适用场景

该方案适合读写并发较大、数据量不是非常大的场景。

##### 5.优缺点点

优点：1 ）可以在任意节点上进行读 2 ）自动剔除故障节点 3 ）自动加入新节点 4 ） 真正并行的复制，基于行级5 ）客 户端连接跟操作单数据库的体验一致。 6) 同步复制，因此具有较高的性能和可靠性。

 缺点： 1) DELETE 操作不支持没有主键的表 , 没有主键的表在不同的节点顺序将不同 2 ）处理事务时， 会运行一个协调认证程序来保证事务的全局一致性 ，若 该 事 务长时间 运行，就会 锁 死节点中所有的相关表，导致插入卡住（这种情况和单表插入是一样的）。 2 ）整个集群的写入吞吐量是由最弱的节点限制，如果有一个节点变得缓慢，那么整个集群将是缓慢的。为了稳定的高性能要求，所有的节点应使用统一的硬件。 3 ）如果 DDL 语句有问题将破坏集群 ,建议禁用。4) Mysql 数据库 5.7.6 及之后的版本才支持此种方案。

## 高读写并发、高数据量方案

### 方案五  数据库中间件

##### 1.数据库架构图

![][6]

##### 2.特点

采用 Mycat 进行分片存储，可以解决写负载均衡和数据量过大问题；每个分片配置多个读从库，可以减少单个库的读压力。

##### 3.开发说明

 此种情况，需要配置 Haproxy 、 keepalive 和 mycat 集群，每个分片上又需要配置一主多从的集群。每个分片上的完整配置，具体请参考方案三，可以简单地把方案三理解为一个分片结构。因此，配置和维护量都比较大。

##### 4.适用场景

 读写并发都很大并且数据量非常大的场景。

##### 5.优缺点

 优点：终极的解决高并发高数据量的方法。

 缺点：配置和维护都比较麻烦，需要的软硬件设备资源大。

[0]: http://www.cnblogs.com/devinzhang/p/7001424.html
[1]: ../img/2004089354.png
[2]: ../img/514892955.png
[3]: ../img/816373026.png
[4]: ../img/1701194230.png
[5]: ../img/599907703.png
[6]: ../img/413688614.png